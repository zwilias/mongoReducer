# Generating statistics from the apache accesslog #

Prerequisites: an apache accesslog. We use one in the _combined_ format in this example, but it can be adapted to something entirely different.

## The goal ##

Creating updated statistics like counting the number of requests on a day by day basis. This information should be consistently up to date. In order to do so, we'll combine a number of techniques that all adhere to our "do it all in mongo"-mindset.

In this example, we'll:

- continously pipe the accesslog into a mongo collection
- define some functionality for analyzing entries in order to split them into pieces
- define a map-reduce action that will periodically analyze those entries and run some map-reduce-fu on them
- look at the generated data, which could be used to, for example, create graphs

## Setup ##

A default `mongo` install comes with a handy little `mongoimport` tool. It's capable of importing data from JSON, CSV or TSV files. Since an apache accesslog isn't any of those, we'll just import each line as a separate entry in our collection and run some "split it into fields" code afterwards.

Since the `mongoimport` tool can read from STDIN, we can also easily set up a `tail` with the follow-option in order to pass data from the accesslog into the mongoimport tool as it is generated.

_Note:_ Naturally, there are tools that were specifically created for doing exactly this, such as [Fluentd](http://docs.fluentd.org/articles/apache-to-mongodb). However, for the purposes of this example, we'll only rely on standard tools available on any linux, unix or mac OS X installation, and mongo.

We'll write to the `accesslog` collection in the `test` db.

The command will look something like this:

```bash
$ tail -f /path/to/access.log | mongoimport -d test -c accesslog -f line --type tsv &
```

Now that that's running, let's move on to setting up some analysis functionality.

## Analysis ##

For the analysis, we'll use a little script you can find in `entryParser.js`. It defines 2 functions - `parseLine(line, parser)` and `parseCollection(collection, parser)`.

Both functions expect a `parser` object. This parser object is a simple object literal that contains a series of _patterns_. Each such _pattern_ is another object literal consisting of a `check` - a regex a line should match before it is handled by the contained _extractors_. An _extractor_ is another object literal contained within the `extract` key of each _pattern.

An extractor consists of a `type` - either `regex`, `split` or `function` - a `names` array, and either a `regex` key with a regex object in it with matching groups, a `split` key with a string value by which the line will be split, or a `func` key with a lambda function as its value which will be called with the line being handled as its argument and which should return an array of values contained within that line.

The `names` array will then be used to create an object literal with those names as its value and the values generated by the `split`, `regex` or `func` result as its values. Excluding any of those values is as simple as setting its name to a value that evaluates to false (`""`, `null`, `false`, `0`, `undefined`, `NaN`).

The resulting object literal will then, after each run of an `extractor`, be merged into the final result. In other words, the order in which the extractors are defined *matters*, since each subsequent `extractor` may overwrite key-value pairs defined by a previous `extractor`.

In order to split an entry from a _combined_ Apache accesslog into key-value pairs, we define a parser like this:

```javascript
{
    name: "apache-acceslog",
    patterns: [
        {
            check: /.*/,
            extract: [
                {
                    type: "regex",
                    regex: /^(\S+) (\S+) (\S+) \[([^:]+):(\d+:\d+:\d+) ([^\]]+)\] \"\s*((\S+) (.*?) (\S+)|(.*))\s*\" (\S+) (\S+) "([^"]*)" "([^"]*)"$/,
                    names: ["ip", "identity", "http_user", "date", "time", "timezone", "request", "method", "resource", "http_version", null, "statuscode", "contentsize", "referer", "useragent"]
                }
            ]
        }
    ]
}
```

Note that we also gave a `name` to our parser. This will be used later to retrieve the parser from a collection.

Since the mongoshell, at the time of writing, has some trouble parsing long regular expressions within a json string, we split the definition of the regex and the object literal, and assign the resulting parser to a variable `parser`.

```javascript
var regex = /^(\S+) (\S+) (\S+) \[([^:]+):(\d+:\d+:\d+) ([^\]]+)\] \"\s*((\S+) (.*?) (\S+)|(.*))\s*\" (\S+) (\S+) "([^"]*)" "([^"]*)"$/;
var parser = {
    name: "apache-acceslog",
    patterns: [
        {
            check: /.*/,
            extract: [
                {
                    type: "regex",
                    regex: regex,
                    names: ["ip", "identity", "http_user", "date", "time", "timezone", "request", "method", "resource", "http_version", null, "statuscode", "contentsize", "referer", "useragent"]
                }
            ]
        }
    ]
};
```

Entering the above in the mongoshell yields us with a `parser` variable that contains the defined parser. Let's check this:

```javascript
> parser
{
        "name" : "apache-acceslog",
        "patterns" : [
                {
                        "check" : /.*/,
                        "extract" : [
                                {
                                        "type" : "regex",
                                        "regex" : /^(\S+) (\S+) (\S+) \[([^:]+):(\d+:\d+:\d+) ([^\]]+)\] \"\s*((\S+) (.*?) (\S+)|(.*))\s*\" (\S+) (\S+) "([^"]*)" "([^"]*)"$/,
                                        "names" : [
                                                "ip",
                                                "identity",
                                                "http_user",
                                                "date",
                                                "time",
                                                "timezone",
                                                "request",
                                                "method",
                                                "resource",
                                                "http_version",
                                                null,
                                                "statuscode",
                                                "contentsize",
                                                "referer",
                                                "useragent"
                                        ]
                                }
                        ]
                }
        ]
}
```

Alright. Now, let's load the entryParser.js code.

```javascript
> load("entryParser.js")
true
```

Let's test this on a line from the accesslog.

```javascript
> var line = db.accesslog.findOne().line
> line
127.0.0.1 - - [10/Jul/2012:18:01:40 +0200] "GET /index.html HTTP/1.1" 200 312 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1"
> parseLine(line, parser).result
{
        "ip" : "127.0.0.1",
        "identity" : "-",
        "http_user" : "-",
        "date" : "10/Jul/2012",
        "time" : "18:01:40",
        "timezone" : "+0200",
        "request" : "GET /index.html HTTP/1.1",
        "method" : "GET",
        "resource" : "/index.html",
        "http_version" : "HTTP/1.1",
        "statuscode" : "200",
        "contentsize" : "312",
        "referer" : "-",
        "useragent" : "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1"
}
```

`parseLine` returns an object literal with 3 keys:

```javascript
{
    result: {},
    log: {},
    line: {}
}
```

The `log` key contains a whole lot of detailed information that can be used when debugging broken `parser`'s. Since we're now only interested in the result, we only look at the `result`.

Now that we've verified our parse works on a single entry, let's run it on the entire collection, using the `parseCollection` function.

The `parseCollection` function loops through all the entries in a collection that don't have a `parsed` key set to `true`, taking their `line` value, running it through `parseLine` and inserting the resulting key-value pairs (only those in the `result`) into the original entry in the collection, along with setting the `parsed` and `parsed_at` key-value pairs to `true` and the current timestamp respectively.

The initial run may take quite a while. With the previously defined parser, expect throughput of around 1000 entries/second - especially if you've created an index on the `parsed` field.

```javascript
> parseCollection("accesslog", parser)
{ "time" : 59620, "count" : 75245, "errorcount" : 0, "errors" : [ ] }
> db.accesslog.findOne()
{
        "_id" : ObjectId("51a4679f001f7d0aa3bd7c96"),
        "contentsize" : "312",
        "date" : "10/Jul/2012",
        "http_user" : "-",
        "http_version" : "HTTP/1.1",
        "identity" : "-",
        "ip" : "127.0.0.1",
        "line" : "79.125.113.10 - - [10/Jul/2012:18:01:40 +0200] \"GET /index.html HTTP/1.1\" 200 312 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1\"",
        "method" : "GET",
        "parsed" : true,
        "parsed_at" : 1369827152015,
        "referer" : "-",
        "request" : "GET /index.html HTTP/1.1",
        "resource" : "/index.html",
        "statuscode" : "200",
        "time" : "18:01:40",
        "timezone" : "+0200",
        "useragent" : "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1"
}
```

To make sure we don't lose this valuable patterns, let's save it in the database:

```javascript
> db.parsers.save(parser)
```

## Map-Reduce ##

Now that's we've got a collection set-up, with data continously being added to it, let's create a Map-Reduce action that will count the number of requests on a day-by-day basis.

This is really rather simple.

```javascript
var action = {
    "name": "dailyReqs",
    "collection": "accesslog",
    "interval": 300000,
    "incremental": "parsed_at",
    "map": function() { emit(new Date(this.date).valueOf(), 1) },
    "reduce": function(key, values) { return Array.sum(values); },
    "pre": function() { return parseCollection("accesslog", db.parsers.findOne({name: "apache-accesslog"})); }
}
```

So we define a new map-reduce action, which we call "dailyReqs". It should operate on the `accesslog` collection, run every 5 minutes (or 300.000 milliseconds). There's a timestamp value with key `parsed_at` which can be used to make this an incremental map-reduce action. The `map` function simply takes the `date` field, turns it into an epoch, and uses this as a key to emit '1'. This way, our `reduce` function can be a simple `Array.sum(values)`, making it idempotent.

Now, as a special little extra, we define a `pre`-processing functio, and we make it execute the `parseCollection` function we previously defined, to make sure all the values in the `accesslog` collection are defined when we execute our `mapreduce` command. As the parser, we pass it `db.parsers.findOne{...}` so that we can update the parser definition while the map-reduce Poller loop is running.

Let's test it, to make sure it works:

```javascript
> mapReducer.runAction(action)
1369829092990   ObjectId("51a5eebb9ff461d1739e4ef0")    Preprocessing returned data
1369829092991   ObjectId("51a5eebb9ff461d1739e4ef0")    No output collection/action specified, writing to results.dailyReqs
1369829092992   ObjectId("51a5eebb9ff461d1739e4ef0")    Clearing results.dailyReqs - reset
1369829098142   ObjectId("51a5eebb9ff461d1739e4ef0")    Finished running dailyReqs
{
        "timestamp" : 1369829098142,
        "action" : { ... },
        "result" : { ... },
        "options" : { ... }
}
```

And there we have it, it works!

So, to make it start doing its job, insert it into the database, run it once more, by name, and let it go its merry way.

```javascript
> db.mapreduce.insert(action)
> mapReducer.run("dailyReqs")
...
> start()
```

And there we have it. As our website is being visited, entries are added to the apache accesslog. Those are then piped into mongoimporter, which throws them into a collection. Whenever our mapreduce action is executed, it first parses these new entries, and then run an incremental map-reduce action.

Done.
